---
title: "hyperparameter-tuning"
format: html
---

```{r}
library(tidyverse)
library(tidymodels)
library(glue)
library(powerjoin)
library(vip)
library(baguette)
library(ggthemes)
library(patchwork)
library(ranger)
library(xgboost)
library(visdat)
library(dials)
```
# Read in the data using map, read_delim() and powerjoin::power_full_join()
```{r}
# download data
root  <- 'https://gdex.ucar.edu/dataset/camels/file'

#download the documentation PDF which provides a descriptions for the various columns 
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

# download the .txt files that store the actual data documented in the PDF. Create a vector storing the data types/file names we want to download:

#data from the pdf that we want
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Using glue, we can construct the needed URLs and file names for the data we want to download:

# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')

# asking walk2 to pass the first element of remote_files and the first element of local_files to the download.file function to download the data, and setting quiet = TRUE to suppress output. The process is then iterated for the second element of each vector, and so on.
walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 

#we are join to merge every data.frame in the list (n = 6) by the shared gauge_id column. Since we want to keep all data, we want a full join.
camels <- power_full_join(camels ,by = 'gauge_id')
```


# Clean the data using dplyr, EDA (skimr, visdat, ggpubr), and other means to ensure it is in a good form for modeling.

```{r}
#look at data structure and where na values are
vis_dat(camels)

# EDA visualizion
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```


# Data Splitting
```{r}
#set seed
set.seed(123)

#transform the q_mean column to a log scale. Remember it is error prone to apply transformations to the outcome variable within a recipe
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# split data: use 80% of the data for training and 20% for testing with no stratification
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

#create a 10-fold cross validation dataset to help us evaluate multi-model setups.
camels_cv <- vfold_cv(camels_train, v = 10)
```


# Feature Engineering
```{r}
#create recipe object with recipe(). Based on how you want to predict q_mean and the data should be the training data.

rec <-  recipe(logQmean ~ aridity + pet_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  
  step_log(all_predictors()) %>%
  
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:pet_mean) |> 
  
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes()) %>%
  
  #Don't use gauge_lat and gauge_lon in the recipe as predictors. You can use the step_rm() function to remove themwhile ensureing they persist in any data passed throuhg fit_*.
  
  update_role(gauge_lat, gauge_lon, new_role = "id")

#problem with step_rm: exclude these columns from being used as predictors, but still keep them in the dataset for later (e.g., to join, identify sites, or plot
```


# Resampling and Modeling Data: Training data is now ready to be used for model training and you have a preprocessor that you can iterate over.

Build Resamples: This step is done above int he data splitting portion


# Build 3 Candidate Models: A model is defined by a specification, an engine, and a mode. 
```{r}
#Define a random forest model using the rand_forest function, set engine to ranger and mode to regression
rf_model2 <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

#Define two other models of choice 
#linear model
lm_model2 <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

#boost model
boost_model2 <- boost_tree() %>%
  # define the engine
  set_engine("xgboost") %>%
  # define the mode
  set_mode("regression")

```


#Test the Models

linear regression, xgboost, and random forest models:
```{r}
#Create a workflow object, add recipe, add the model, to model to resamples
wf2 <- workflow_set(list(rec), list(lm_model2, boost_model2,rf_model2)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)

# Use autoplot and rank_results to compare the models.
autoplot(wf2)

rank_results(wf2, rank_metric = "rsq", select_best = TRUE)
```

# Model Selection

Based on the visualized metrics, select a model that you think best performs. Describe the reason for your choice using the metrics.
```{r}
rank_results(wf2, rank_metric = "rsq", select_best = TRUE)
```
The linear regression model is best out of the three models that were run. This is because the PET_mean and aridity showed more of a linear relationship with the outcome variable of the log Qmean. It has a higher mean accuracy and a slightly lower standard deviation of error than the other models.

The engine for the linear regression model is linear, while the mode is regression. This type of model is good for this data set because it's small and easy to train and predict without the risk of over fitting. It also is a more linear relationship, that's better explained by this model than the other two.


# Model Tuning

Define a tunable model: Be sure to set the mode and engine as you did above but this time specify at least 2 hyperparameters to tune using the tune() function. These are set in the model specification and options can be found in the documentation for the model you are using.
```{r}
#check tunable paramters for line reg
?linear_reg

#choosing glmnet:uses regularized least squares to fit models with numeric outcomes.
# For this engine, there is a single mode: regression

#This model has 2 tuning parameters:
#penalty: Amount of Regularization (type: double, default: see below)
#mixture: Proportion of Lasso Penalty (type: double, default: 1.0)

#A value of mixture = 1 corresponds to a pure lasso model, while mixture = 0 indicates ridge regression.

#The penalty parameter has no default and requires a single numeric value.

lm_tunable <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

```


 # Create a workflow
 
Now create a workflow object for this tune-able model and the recipe you created above.
```{r}
# Create the workflow by combining the recipe and model
lm_tuned_wf <- workflow(rec,lm_tunable)
  
```


# Check The Tunable Values / Ranges

In the above step, you created a workflow based on a model that has at least 2 tunable hyper parameters. Remember tune() is aware of the tunable parameters in the model specification and provides some basic defaults to tune across. We want to see what the range of those are! To do this, we can use the extract_parameter_set_dials() to return a list of the tunable parameters and their ranges. The output of this function is a data.frame.

Use the extract_parameter_set_dials(YOUR MODEL WORKFLOW) and save it to an object named dials. Check the dials$object slot to see the tunable parameters and their ranges.
```{r}
# Extract tunable parameters from the workflow
dials <- extract_parameter_set_dials(lm_tuned_wf)

# View the parameter ranges
dials$object

#penalty: Regularization strength (lambda in glmnet); default range is on a log scale from 1e-10 to 1. A higher penalty forces coefficients toward zero (more shrinkage), which can help prevent overfitting. -10 → corresponds to a very small penalty (10^-10 = 1e-10), 0 → corresponds to a penalty of 1 (10^0 = 1).

#mixture: Elastic net mixing value (0 = ridge, 1 = lasso); linear scale from 0 to 1.

#Tuning penalty helps you control overfitting. Tuning mixture helps you balance between:

#Ridge: good when many small effects. Lasso: good when some variables are irrelevant (feature selection)

```

# Define the Search Space


OK, we now know the viable range of the hyperparameters we want to tune. The full range of these values is the complete search space in which the best options could live. Remember, there are two ways for our computer to search for the best combination:

1. Grid Search: This is a brute force approach that tests a predefined set of combinations. If elected as the search process, we need to specify how the set of combinations is chosen. This can be versions of evenly spaced (regular), random, or space filling curve/design (SFC/SFD).

2. Iterative: This is a more efficient approach that uses a set of algorithms to search the space. This is often more efficient than grid search, but can be less thorough.

For this lab, lets use a grid search based on a “Latin Hypercube” SFD method. We can specify this by passing our above dials object to the grid_latin_hypercube() function. The only thing we need to specify is the size of the grid we want to evaluate. The larger the size, the more thorough the search, but the longer your computer will take. Lets set the size to 20 for this lab and save the outputs to an object called my.grid

Create a SFD Grid Object with 25 predefined combinations.
```{r}
# Create a Latin Hypercube grid with 25 combinations
my.grid <- lm_tuned_wf %>%
  extract_parameter_set_dials %>%
  grid_latin_hypercube(size=25)

#size = 25 tells it to generate 25 unique combinations of penalty and mixture.

#grid_latin_hypercube() samples evenly across the search space, but in a way that covers more ground than a regular grid with fewer points.
```


