---
title: "hyperparameter-tuning"
format: html
---

```{r}
library(tidyverse)
library(tidymodels)
library(glue)
library(powerjoin)
library(vip)
library(baguette)
library(ggthemes)
library(patchwork)
library(ranger)
library(xgboost)
library(visdat)
```
# Read in the data using map, read_delim() and powerjoin::power_full_join()
```{r}
# download data
root  <- 'https://gdex.ucar.edu/dataset/camels/file'

#download the documentation PDF which provides a descriptions for the various columns 
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

# download the .txt files that store the actual data documented in the PDF. Create a vector storing the data types/file names we want to download:

#data from the pdf that we want
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Using glue, we can construct the needed URLs and file names for the data we want to download:

# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')

# asking walk2 to pass the first element of remote_files and the first element of local_files to the download.file function to download the data, and setting quiet = TRUE to suppress output. The process is then iterated for the second element of each vector, and so on.
walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 

#we are join to merge every data.frame in the list (n = 6) by the shared gauge_id column. Since we want to keep all data, we want a full join.
camels <- power_full_join(camels ,by = 'gauge_id')
```


# Clean the data using dplyr, EDA (skimr, visdat, ggpubr), and other means to ensure it is in a good form for modeling.

```{r}
#look at data structure and where na values are
vis_dat(camels)

# EDA visualizion
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```


# Data Splitting
```{r}
#set seed
set.seed(123)

#transform the q_mean column to a log scale. Remember it is error prone to apply transformations to the outcome variable within a recipe
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# split data: use 80% of the data for training and 20% for testing with no stratification
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

#create a 10-fold cross validation dataset to help us evaluate multi-model setups.
camels_cv <- vfold_cv(camels_train, v = 10)
```


# Feature Engineering
```{r}
#create recipe object with recipe(). Based on how you want to predict q_mean and the data should be the training data.

rec <-  recipe(logQmean ~ aridity + pet_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  
  step_log(all_predictors()) %>%
  
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:pet_mean) |> 
  
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes()) %>%
  
  #Don't use gauge_lat and gauge_lon in the recipe as predictors. You can use the step_rm() function to remove themwhile ensureing they persist in any data passed throuhg fit_*.
  
  update_role(gauge_lat, gauge_lon, new_role = "id")

#problem with step_rm: exclude these columns from being used as predictors, but still keep them in the dataset for later (e.g., to join, identify sites, or plot
```


# Resampling and Modeling Data: Training data is now ready to be used for model training and you have a preprocessor that you can iterate over.

Build Resamples: This step is done above int he data splitting portion


# Build 3 Candidate Models: A model is defined by a specification, an engine, and a mode. 
```{r}
#Define a random forest model using the rand_forest function, set engine to ranger and mode to regression
rf_model2 <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

#Define two other models of choice 
#linear model
lm_model2 <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

#boost model
boost_model2 <- boost_tree() %>%
  # define the engine
  set_engine("xgboost") %>%
  # define the mode
  set_mode("regression")

```


#Test the Models

linear regression, xgboost, and random forest models:
```{r}
#Create a workflow object, add recipe, add the model, to model to resamples
wf2 <- workflow_set(list(rec), list(lm_model2, boost_model2,rf_model2)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)

# Use autoplot and rank_results to compare the models.
autoplot(wf2)

rank_results(wf2, rank_metric = "rsq", select_best = TRUE)
```

# Model Selection

Based on the visualized metrics, select a model that you think best performs. Describe the reason for your choice using the metrics.
```{r}
rank_results(wf2, rank_metric = "rsq", select_best = TRUE)
```
The linear regression model is best out of the three models that were run. This is because the PET_mean and aridity showed more of a linear relationship with the outcome variable of the log Qmean. It has a higher mean accuracy and a slightly lower standard deviation of error than the other models.

The engine for the linear regression model is linear, while the mode is regression. This type of model is good for this data set because it's small and easy to train and predict without the risk of over fitting. It also is a more linear relationship, that's better explained by this model than the other two.


# Model Tuning

Define a tunable model: Be sure to set the mode and engine as you did above but this time specify at least 2 hyperparameters to tune using the tune() function. These are set in the model specification and options can be found in the documentation for the model you are using.
```{r}

```


