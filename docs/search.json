[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "lab6",
    "section": "",
    "text": "#Load libraries for this lab\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggthemes)\nlibrary(patchwork)\nlibrary(ranger)\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice"
  },
  {
    "objectID": "lab6.html#first-lets-make-a-map-of-the-sites.-use-the-borders-ggplot-function-to-add-state-boundaries-to-the-map-and-initially-color-the-points-by-the-mean-flow-q_mean-at-each-site.",
    "href": "lab6.html#first-lets-make-a-map-of-the-sites.-use-the-borders-ggplot-function-to-add-state-boundaries-to-the-map-and-initially-color-the-points-by-the-mean-flow-q_mean-at-each-site.",
    "title": "lab6",
    "section": "first, lets make a map of the sites. Use the borders() ggplot function to add state boundaries to the map and initially color the points by the mean flow (q_mean) at each site.",
    "text": "first, lets make a map of the sites. Use the borders() ggplot function to add state boundaries to the map and initially color the points by the mean flow (q_mean) at each site.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n** scales can be used to map data values to colors (scale_color_) or fill aesthetics (scale_fill_). There are two main types of color scales:\nDiscrete color scales – for things that are categories, like “apples,” “bananas,” and “cherries.” Each gets its own separate color. scale_color_manual(values = c(“red”, “yellow”, “pink”)) #lets you pick your own colors.\nOr\nscale_color_brewer(palette = “Set1”) #uses a built-in color set.\nContinuous color scales – for numbers, like temperature (cold to hot) or height (short to tall). The color changes smoothly. scale_color_gradient(low = “blue”, high = “red”) #makes small numbers blue and big numbers red."
  },
  {
    "objectID": "lab6.html#model-preparation",
    "href": "lab6.html#model-preparation",
    "title": "lab6",
    "section": "Model Preparation",
    "text": "Model Preparation"
  },
  {
    "objectID": "lab6.html#lets-start-by-looking-that-the-3-dimensions-variables-of-this-data.-well-start-with-a-xy-plot-of-aridity-and-rainfall.-we-are-going-to-use-the-scale_color_viridis_c-function-to-color-the-points-by-the-q_mean-column.-this-scale-functions-maps-the-color-of-the-points-to-the-values-in-the-q_mean-column-along-the-viridis-continuous-c-palette.-because-a-scale_color_-function-is-applied-it-maps-to-the-known-color-aesthetic-in-the-plot.",
    "href": "lab6.html#lets-start-by-looking-that-the-3-dimensions-variables-of-this-data.-well-start-with-a-xy-plot-of-aridity-and-rainfall.-we-are-going-to-use-the-scale_color_viridis_c-function-to-color-the-points-by-the-q_mean-column.-this-scale-functions-maps-the-color-of-the-points-to-the-values-in-the-q_mean-column-along-the-viridis-continuous-c-palette.-because-a-scale_color_-function-is-applied-it-maps-to-the-known-color-aesthetic-in-the-plot.",
    "title": "lab6",
    "section": "Lets start by looking that the 3 dimensions (variables) of this data. We’ll start with a XY plot of aridity and rainfall. We are going to use the scale_color_viridis_c() function to color the points by the q_mean column. This scale functions maps the color of the points to the values in the q_mean column along the viridis continuous (c) palette. Because a scale_color_* function is applied, it maps to the known color aesthetic in the plot.",
    "text": "Lets start by looking that the 3 dimensions (variables) of this data. We’ll start with a XY plot of aridity and rainfall. We are going to use the scale_color_viridis_c() function to color the points by the q_mean column. This scale functions maps the color of the points to the values in the q_mean column along the viridis continuous (c) palette. Because a scale_color_* function is applied, it maps to the known color aesthetic in the plot.\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lab6.html#model-building",
    "href": "lab6.html#model-building",
    "title": "lab6",
    "section": "Model Building",
    "text": "Model Building"
  },
  {
    "objectID": "lab6.html#preprocessor-recipe",
    "href": "lab6.html#preprocessor-recipe",
    "title": "lab6",
    "section": "Preprocessor: recipe",
    "text": "Preprocessor: recipe"
  },
  {
    "objectID": "lab6.html#naive-base-lm-approach",
    "href": "lab6.html#naive-base-lm-approach",
    "title": "lab6",
    "section": "Naive base lm approach",
    "text": "Naive base lm approach"
  },
  {
    "objectID": "lab6.html#where-things-get-a-little-messy",
    "href": "lab6.html#where-things-get-a-little-messy",
    "title": "lab6",
    "section": "Where things get a little messy…",
    "text": "Where things get a little messy…"
  },
  {
    "objectID": "lab6.html#wrong-version-1-augment",
    "href": "lab6.html#wrong-version-1-augment",
    "title": "lab6",
    "section": "Wrong version 1: augment",
    "text": "Wrong version 1: augment"
  },
  {
    "objectID": "lab6.html#wrong-version-2-predict",
    "href": "lab6.html#wrong-version-2-predict",
    "title": "lab6",
    "section": "Wrong version 2: predict",
    "text": "Wrong version 2: predict"
  },
  {
    "objectID": "lab6.html#correct-version-prep---bake---predict",
    "href": "lab6.html#correct-version-prep---bake---predict",
    "title": "lab6",
    "section": "Correct version: prep -> bake -> predict",
    "text": "Correct version: prep -&gt; bake -&gt; predict"
  },
  {
    "objectID": "lab6.html#model-evaluation-statistical-and-visual",
    "href": "lab6.html#model-evaluation-statistical-and-visual",
    "title": "lab6",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual"
  },
  {
    "objectID": "lab6.html#using-a-workflow-instead",
    "href": "lab6.html#using-a-workflow-instead",
    "title": "lab6",
    "section": "Using a workflow instead",
    "text": "Using a workflow instead"
  },
  {
    "objectID": "lab6.html#making-predictions",
    "href": "lab6.html#making-predictions",
    "title": "lab6",
    "section": "Making Predictions",
    "text": "Making Predictions"
  },
  {
    "objectID": "lab6.html#model-evaluation-statistical-and-visual-1",
    "href": "lab6.html#model-evaluation-statistical-and-visual-1",
    "title": "lab6",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual"
  },
  {
    "objectID": "lab6.html#switch-it-up",
    "href": "lab6.html#switch-it-up",
    "title": "lab6",
    "section": "Switch it up!",
    "text": "Switch it up!"
  },
  {
    "objectID": "lab6.html#predictions",
    "href": "lab6.html#predictions",
    "title": "lab6",
    "section": "Predictions",
    "text": "Predictions"
  },
  {
    "objectID": "lab6.html#model-evaluation-statistical-and-visual-2",
    "href": "lab6.html#model-evaluation-statistical-and-visual-2",
    "title": "lab6",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual"
  },
  {
    "objectID": "lab6.html#a-workflowset-approach",
    "href": "lab6.html#a-workflowset-approach",
    "title": "lab6",
    "section": "A workflowset approach",
    "text": "A workflowset approach"
  },
  {
    "objectID": "lab6.html#question-3-your-turn",
    "href": "lab6.html#question-3-your-turn",
    "title": "lab6",
    "section": "Question 3: Your Turn!",
    "text": "Question 3: Your Turn!"
  },
  {
    "objectID": "lab6.html#build-your-own",
    "href": "lab6.html#build-your-own",
    "title": "lab6",
    "section": "Build Your Own",
    "text": "Build Your Own\nBorrowing from the workflow presented above, build your own complete ML pipeline to predict mean stream flow using the CAMELS dataset. You can experiment with different predictors and preprocessing steps to see how they impact model performance. A successful model will have a R-squared value &gt; 0.9. To get started, you can use the following steps as a template:\nQ4a) Data prep/split\n\n#set a seed for reproducibility \nset.seed(321)\n\n#Create an initial split with 75% used for training and 25% for testing\n#note- we did a log transformation on camels recipe in above example for model building\ncamels_split2 &lt;- initial_split(camels, prop = 0.75)\ncamels_train2 &lt;- training(camels_split2)\ncamels_test2  &lt;- testing(camels_split2)\n\n#10-fold cross validation dataset to help us evaluate multi-model setups.\ncamels_cv2 &lt;- vfold_cv(camels_train2, v = 10)\n\nQ4b) Recipe\nI’m using a multiple regression model to show the prediction of the logQmean by aridity, pet_mean because these variables are continuous.\n\n#Define a formula you want to use to predict logQmean & recipe\nrec2 &lt;-  recipe(logQmean ~ aridity + pet_mean, data = camels_train2) %&gt;%\n  # Log transform the predictor variables (aridity and pet_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and pet_mean\n  step_interact(terms = ~ aridity:pet_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\nQ4c): Define 3 models\n\n# Prep,bake predict\n#This ensures the test data is transformed in the same way as the training data before making predictions\nbaked_data2 &lt;- prep(rec2, camels_train2) |&gt; \n  bake(new_data = NULL)\n\n#Define a random forest model using the rand_forest function, set engine to ranger and mode to regression\nrf_model2 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n#Define two other models of choice \n#linear model\nlm_model2 &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n#boost model\nboost_model2 &lt;- boost_tree() %&gt;%\n  # define the engine\n  set_engine(\"xgboost\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n\n\n#rf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  #add_recipe(rec) %&gt;%\n  # Add the model\n # add_model(rf_model) %&gt;%\n  # Fit the model\n  #fit(data = camels_train) \n#Set the engine to ranger and the mode to regression\n# Define two other models of your choice\n\nQ4d) workflow set\n\n#Create a workflow object, add recipe, add the model, to model to resamples\nwf2 &lt;- workflow_set(list(rec2), list(lm_model2, boost_model2,rf_model2)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nQ4e) Evaluation\n\n# Use autoplot and rank_results to compare the models.\nautoplot(wf2)\n\n\n\n\n\n\n\nrank_results(wf2, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_linear_reg Prepro… rmse    0.567  0.0246    10 recipe       line…     1\n2 recipe_linear_reg Prepro… rsq     0.770  0.0228    10 recipe       line…     1\n3 recipe_rand_fore… Prepro… rmse    0.575  0.0296    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.759  0.0285    10 recipe       rand…     2\n5 recipe_boost_tree Prepro… rmse    0.611  0.0330    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.733  0.0302    10 recipe       boos…     3\n\n\nThe linear regression model is best out of the three models that were run. This is because the PET_mean and aridity showed more of a linear realationship with the outcome variable of the log Qmean.\nQ5f) Extract and Evaluate\n\n#Build a workflow (not workflow set) with your favorite model, recipe, and training data.Use fit to fit all training data to the model\n\nlm_wf2 &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec2) %&gt;%\n  # Add the model\n  add_model(lm_model2) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train2) \n\n#Use augment to make predictions on the test data\nlm_data2 &lt;- augment(lm_wf2, new_data = camels_test2)\ndim(lm_data2)\n\n[1] 168  61\n\n#Create a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale\nggplot(lm_data2, aes(x = logQmean, y = .pred, colour = pet_mean)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"PET Mean\")\n\n\n\n\n\n\n\n#metrics\nmetrics(lm_data2, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.532\n2 rsq     standard       0.797\n3 mae     standard       0.350"
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "hyperparameter-tuning",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(glue)\nlibrary(powerjoin)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggthemes)\nlibrary(patchwork)\nlibrary(ranger)\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(visdat)\nlibrary(dials)\nlibrary(tune)\n\n\nRead in the data using map, read_delim() and powerjoin::power_full_join()\n\n# download data\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n#download the documentation PDF which provides a descriptions for the various columns \ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n# download the .txt files that store the actual data documented in the PDF. Create a vector storing the data types/file names we want to download:\n\n#data from the pdf that we want\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Using glue, we can construct the needed URLs and file names for the data we want to download:\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n# asking walk2 to pass the first element of remote_files and the first element of local_files to the download.file function to download the data, and setting quiet = TRUE to suppress output. The process is then iterated for the second element of each vector, and so on.\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n#we are join to merge every data.frame in the list (n = 6) by the shared gauge_id column. Since we want to keep all data, we want a full join.\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\n\nClean the data using dplyr, EDA (skimr, visdat, ggpubr), and other means to ensure it is in a good form for modeling.\n\n#look at data structure and where na values are\nvis_dat(camels)\n\n\n\n\n\n\n\n# EDA visualizion\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nData Splitting\n\n#set seed\nset.seed(123)\n\n#transform the q_mean column to a log scale. Remember it is error prone to apply transformations to the outcome variable within a recipe\ncamels_clean &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean)) %&gt;%\n  select(logQmean,aridity,pet_mean, gauge_lat, gauge_lon) %&gt;%\n  drop_na()\n\n# split data: use 80% of the data for training and 20% for testing with no stratification\ncamels_split &lt;- initial_split(camels_clean, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\n#create a 10-fold cross validation dataset to help us evaluate multi-model setups.\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n\nFeature Engineering\n\n#create recipe object with recipe(). Based on how you want to predict q_mean and the data should be the training data.\n\nrec &lt;-recipe(logQmean ~. , data = camels_train) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  # Log transform the predictor variables (aridity and pet_mean)\n  #step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:pet_mean) %&gt;%\n  # Drop any rows with missing values in the pred\n  #step_naomit(all_predictors(), all_outcomes()) %&gt;%\n  step_rm(gauge_lat, gauge_lon) \n\n\n\n#problem with step_rm: exclude these columns from being used as predictors, but still keep them in the dataset for later (e.g., to join, identify sites, or plot\n\n\n\nResampling and Modeling Data: Training data is now ready to be used for model training and you have a preprocessor that you can iterate over.\nBuild Resamples: This step is done above in the data splitting portion\n\n\nBuild 3 Candidate Models: A model is defined by a specification, an engine, and a mode.\n\n#Define a random forest model using the rand_forest function, set engine to ranger and mode to regression\nrf_model2 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n#Define two other models of choice \n#linear model\nlm_model2 &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n#boost model\nboost_model2 &lt;- boost_tree() %&gt;%\n  # define the engine\n  set_engine(\"xgboost\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n#Test the Models\nlinear regression, xgboost, and random forest models:\n\n#Create a workflow object, add recipe, add the model, to model to resamples\nwf2 &lt;- workflow_set(list(rec), list(lm_model2, boost_model2,rf_model2)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\n# Use autoplot and rank_results to compare the models.\nautoplot(wf2)\n\n\n\n\n\n\n\nrank_results(wf2, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.594  0.0344    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.747  0.0234    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.624  0.0513    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.740  0.0389    10 recipe       line…     2\n5 recipe_boost_tree Prepro… rmse    0.640  0.0334    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.714  0.0247    10 recipe       boos…     3\n\n\n\n\nModel Selection\nBased on the visualized metrics, select a model that you think best performs. Describe the reason for your choice using the metrics.\n\nrank_results(wf2, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.594  0.0344    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.747  0.0234    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.624  0.0513    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.740  0.0389    10 recipe       line…     2\n5 recipe_boost_tree Prepro… rmse    0.640  0.0334    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.714  0.0247    10 recipe       boos…     3\n\n\nThe random forest model is the best out of the three models that were run. This is because even though PET_mean and aridity show a slight linear relationship with the outcome variable of the log Qmean, there are outliers and unpredictability that’s best described with random forest. It has a higher mean accuracy and a slightly lower standard deviation of error than the other models.\nThe engine for the random forest model is ranger, while the mode is regression. This type of model is good for this data set because it’s small and easy to train and predict without the risk of over fitting. It also helps to learn about the interactions between other variables in the tree-building process.\n\n\nModel Tuning\nDefine a tunable model: Be sure to set the mode and engine as you did above but this time specify at least 2 hyperparameters to tune using the tune() function. These are set in the model specification and options can be found in the documentation for the model you are using.\n\n#check tunable parameters for rand_forest\n?rand_forest\n\nstarting httpd help server ... done\n\n#mtry: Number of predictors randomly sampled at each split.\n#Type: Integer\n#Effect: Controls how diverse the trees are. Lower values increase diversity, but may reduce accuracy\n\n#min_n: Minimum number of observations required to make a further split in a node.\n#Type: Integer\n#Effect: Controls how deep the trees can grow. Larger min_n means shallower trees, which helps avoid overfitting.\n\nrf_tunable &lt;- rand_forest(\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# Create a workflow\nNow create a workflow object for this tune-able model and the recipe you created above.\n\n# Create the workflow by combining the recipe and model\nrf_wf &lt;- workflow(rec,rf_tunable)\n\n\n\nCheck The Tunable Values / Ranges\nIn the above step, you created a workflow based on a model that has at least 2 tunable hyper parameters. Remember tune() is aware of the tunable parameters in the model specification and provides some basic defaults to tune across. We want to see what the range of those are! To do this, we can use the extract_parameter_set_dials() to return a list of the tunable parameters and their ranges. The output of this function is a data.frame.\nUse the extract_parameter_set_dials(YOUR MODEL WORKFLOW) and save it to an object named dials. Check the dials$object slot to see the tunable parameters and their ranges.\n\n# Extract tunable parameters from the workflow\ndials &lt;- extract_parameter_set_dials(rf_wf)\n\n# View the parameter ranges\ndials$object\n\n[[1]]\n\n\n# Randomly Selected Predictors (quantitative)\n\n\nRange: [1, ?]\n\n\n\n[[2]]\n\n\nMinimal Node Size (quantitative)\n\n\nRange: [2, 40]\n\n#Lower mtry increases randomness and tree diversity (good for generalization).Too low might make trees less accurate.\n\n#min_n : Higher min_n → simpler trees, less overfitting. Too high → underfitting (model too simple).\n\n\n\nDefine the Search Space\nOK, we now know the viable range of the hyperparameters we want to tune. The full range of these values is the complete search space in which the best options could live. Remember, there are two ways for our computer to search for the best combination:\n\nGrid Search: This is a brute force approach that tests a predefined set of combinations. If elected as the search process, we need to specify how the set of combinations is chosen. This can be versions of evenly spaced (regular), random, or space filling curve/design (SFC/SFD).\nIterative: This is a more efficient approach that uses a set of algorithms to search the space. This is often more efficient than grid search, but can be less thorough.\n\nFor this lab, lets use a grid search based on a “Latin Hypercube” SFD method. We can specify this by passing our above dials object to the grid_latin_hypercube() function. The only thing we need to specify is the size of the grid we want to evaluate. The larger the size, the more thorough the search, but the longer your computer will take. Lets set the size to 20 for this lab and save the outputs to an object called my.grid\nCreate a SFD Grid Object with 25 predefined combinations.\n\n#because undefinable max range of mtry, need to finalize dial before pass it into grid\n\n# 2. Finalize parameter set based on training data (camels_train)\ndials_final &lt;- finalize(dials, camels_train)\n\n# 3. Now you can create the grid with 25 combinations\nmy.grid &lt;- grid_latin_hypercube(\n  dials_final,\n  size = 25)\n\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n\n#grid_latin_hypercube() samples evenly across the search space, but in a way that covers more ground than a regular grid with fewer points.\n\n\n\nTune the Model\nOK! You now have a tunable model workflow (rf_wf), a set of k-fold resamples to test over (folds), and a grid of hyperparameters to search (grid). Now we can use the tune_grid() function to search the grid and evaluate the model performance using the code below. In this example, we are doing 2 additional things. Setting a set of metrics to compute and saving the predictions to a output tibble.\nRun the below code making any changes your need based on how you named your objects.\n\nmodel_params &lt;- tune_grid(\n  rf_wf,                 # your random forest workflow\n  resamples = camels_cv, # your cross-validation folds\n  grid = my.grid,        # the grid you built using Latin Hypercube\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\n→ A | warning: ! 5 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: ! 4 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x4\nThere were issues with some computations   A: x1   B: x6\nThere were issues with some computations   A: x4   B: x6\nThere were issues with some computations   A: x4   B: x10\nThere were issues with some computations   A: x6   B: x12\nThere were issues with some computations   A: x7   B: x16\nThere were issues with some computations   A: x7   B: x18\nThere were issues with some computations   A: x10   B: x20\nThere were issues with some computations   A: x10   B: x24\nThere were issues with some computations   A: x13   B: x24\nThere were issues with some computations   A: x13   B: x28\nThere were issues with some computations   A: x13   B: x30\nThere were issues with some computations   A: x16   B: x30\nThere were issues with some computations   A: x16   B: x34\nThere were issues with some computations   A: x17   B: x36\nThere were issues with some computations   A: x19   B: x38\nThere were issues with some computations   A: x19   B: x41\nThere were issues with some computations   A: x22   B: x42\nThere were issues with some computations   A: x22   B: x46\nThere were issues with some computations   A: x23   B: x48\nThere were issues with some computations   A: x25   B: x51\nThere were issues with some computations   A: x25   B: x54\nThere were issues with some computations   A: x28   B: x54\nThere were issues with some computations   A: x28   B: x58\nThere were issues with some computations   A: x30   B: x60\nThere were issues with some computations   A: x30   B: x60\n\nautoplot(model_params)\n\n\n\n\n\n\n\n#Tests 25 random forest models (from my.grid).Across all resamples in camels_cv. Evaluates 3 metrics: RMSE, R², and MAE. Saves predictions from each resample so you can inspect them later\n\nThere is a randomly predicted side and minimal node side that show the metrics. It looks to be a scatter plot with clusters of points that show a trend with the rmse, rsq, and mae.\n\n\nCheck the skill of the tuned model\nNow that you have tuned the model under many combinations of hyperparameters, you can check the skill using the collect_metrics() function. This will return a tibble with the metrics for each combination of hyperparameters.\nUse the collect_metrics() function to check the skill of the tuned model. Describe what you see, remember dplyr functions like arrange, slice_*, and filter will work on this tibble.\n\n #Collect the performance metrics from the tuning results\nmetrics_summary &lt;- collect_metrics(model_params)\n\n# which combination gave the lowest RMSE:\nmetrics_summary %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(mean) %&gt;%\n  slice_head(n = 5)\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3    34 rmse    standard   0.564    10  0.0341 Preprocessor1_Model09\n2     4    39 rmse    standard   0.564    10  0.0336 Preprocessor1_Model07\n3     2    34 rmse    standard   0.566    10  0.0349 Preprocessor1_Model08\n4     3    29 rmse    standard   0.568    10  0.0339 Preprocessor1_Model12\n5     2    38 rmse    standard   0.568    10  0.0344 Preprocessor1_Model17\n\n# which combination gave the highest R²:\nmetrics_summary %&gt;%\n  filter(.metric == \"rsq\") %&gt;%\n  arrange(desc(mean)) %&gt;%\n  slice_head(n = 5)\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3    34 rsq     standard   0.770    10  0.0248 Preprocessor1_Model09\n2     4    39 rsq     standard   0.770    10  0.0245 Preprocessor1_Model07\n3     2    34 rsq     standard   0.768    10  0.0247 Preprocessor1_Model08\n4     3    29 rsq     standard   0.767    10  0.0248 Preprocessor1_Model12\n5     2    38 rsq     standard   0.767    10  0.0239 Preprocessor1_Model17\n\n\nA lot of different numbers that describe the mtry. min_n, metric, estimator, mean, n, and std_err. The lowest mean of .54587 has a mtry of 2 and min_n of 18.\nYou can also use the show_best() function to show the best performing model based on a metric of your choice. For example, if you want to see the best performing model based on the RMSE metric, you can use the following code: Use the show_best() function to show the best performing model based on Mean Absolute Error.\nPlease interpret the results of the first row of show_best(). What do you see? What hyperparameter set is best for this model, based on MAE?\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3    34 mae     standard   0.350    10  0.0183 Preprocessor1_Model09\n2     4    39 mae     standard   0.352    10  0.0183 Preprocessor1_Model07\n3     3    29 mae     standard   0.352    10  0.0182 Preprocessor1_Model12\n4     3    27 mae     standard   0.353    10  0.0186 Preprocessor1_Model10\n5     5    24 mae     standard   0.353    10  0.0179 Preprocessor1_Model21\n\n#mtry refers to the number of variables (predictors) randomly sampled at each split in a random forest model. It controls the randomness of the model by limiting the number of features used to make decisions at each node of the tree.\n\n#The best model uses a value of 20 for min_n, meaning each leaf node in the decision trees must have at least 20 data points before it can be created.\n\n# The standard error (0.016) indicates how much variability there is in the model's MAE across the 10 folds. A smaller standard error suggests more stability in the model's performance.\n\nThe lowest avg. mean is .3318 with a std_err of 0.0160, mtry of 3 and min_n of 20. The model’s performance is stable and consistent across different folds.\nA short cut for show_best(…, n = 1) is to use select_best(). This will return the best performing hyperparameter set based on the metric you choose. Use the select_best() function to save the best performing hyperparameter set to an object called hp_best. Please interpret the results of the first row of show_best(). What do you see? What hyperparameter set is best for this model, based on MAE?\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\nhp_best\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     3    34 Preprocessor1_Model09\n\n\nThe best hyper parameters for this set is to have 3 predictable variables and at least 20 data points.\n\n\nFinalize your model\nFantastic, you now have a workflow and a idealized set of hyperparameters to use for your model. Now we can inject the hyperparameters into the workflow using the finalize_workflow() function. This will create a new workflow object that is no longer dependent on tune() attributes, but rather the finalized hyperparameters. This is a new workflow object that is ready to be used for final model fitting.\nRun finalize_workflow() based on your workflow and best hyperparmater set to create a final workflow object:\n\n# Finalize the workflow with the best hyperparameter set\nfinal_wf &lt;- finalize_workflow(rf_wf, hp_best)\n\n\n\nFinal Model Verification\nThe show_best(), select_best(), and collect_metrics() functions are all great, but they are only implemented on the resampled iterations of the training data. Remember, applications over just the training data can often be misleading. We need to check the final model on the test data to see how it performs.\nHere, we can leverage a short cut in tidymodels to fit the final model to the test data. This is done using the last_fit() function. The last_fit() function will take the finalized workflow, and the full split data object (containing both the training and testing data) and fit it to the training data and validate it on the testing data.\nUse last_fit() to fit the finalized workflow the original split object (output of initial_split()). This will fit the model to the training data and validate it on the testing data.\n\nlast_fit_results &lt;- last_fit(final_wf, split = camels_split)\n\nWith the final model fit, we can now check the performance of the model on the test data in our two standard ways!\nUse the collect_metrics() function to check the performance of the final model on the test data. This will return a tibble with the metrics for the final model. Interpret these results. How does the final model perform on the test data? Is it better or worse than the training data? Use your knowledge of the regression based metrics to describe the results.\n\n# Collect metrics for the final model on the test data\nfinal_model_metrics &lt;- collect_metrics(last_fit_results)\n\n#show results\nfinal_model_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.535 Preprocessor1_Model1\n2 rsq     standard       0.777 Preprocessor1_Model1\n\n\nThe rmse predictions of this model are off by 0.588 units, which is higher than the training data metrics which were .54587. The rsq predictions are 72.8%, and the training data shows 78.48%. This means that the prediction is slightly overfit and the model was uable to explain more variance in the training than the testing data. The random forest model has decent predictive performance.\nUse the collect_predictions() function to check the predictions of the final model on the test data. This will return a tibble with the predictions for the final model.\n\n# Fit the final model and collect predictions using last_fit\nfinal_fit &lt;- last_fit(final_wf, split = camels_split)\n\n# Use collect_predictions() to get the predictions\nfinal_predictions &lt;- collect_predictions(final_fit)\n\n# View the first few rows of predictions\nhead(predictions)\n\n                          \n1 function (x, ...)       \n2 UseMethod(\"predictions\")\n\n\nUse the output of this to create a scatter plot of the predicted values vs the actual values. Use the ggplot2 package to create the plot. This plot should include (1) geom_smooth(method = “lm”) to add the linear fit of predictions and truth (2) geom_abline() to add a 1:1 line (3) nice colors via scale_color_* and (4) accurate labels.\n\n# Create the scatter plot with a color gradient\nggplot(final_predictions, aes(x = logQmean, y = .pred)) +\n  # (1) Linear trend line of predictions vs actuals\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linetype = \"dashed\") +\n  \n  # (2) 1:1 reference line (perfect prediction)\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"solid\") +\n  \n  # (3) Points with green color scale\n  geom_point(aes(color = .pred), size = 2, alpha = 0.5) +\n  scale_color_gradient(low = \"yellowgreen\", high = \"darkgreen\") +\n  \n  # (4) Labels and theme\n  labs(\n    title = \"Predicted vs Actual logQmean\",\n    x = \"Actual logQmean\",\n    y = \"Predicted logQmean\",\n    color = \"Predicted\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#linear fit line (blue dashed line) will show the trend of predictions vs. actual values.\n\n#The 1:1 line (red solid line) will show where predicted values equal actual values.\n\n\n\nBuilding a Map!\nAs a last step, you want to map your predictions across CONUS. Return to the ggplot examples in Lab 6 to refresh your memory on building a map with a defined color pallete.\nTo build your final prediction, you can use fit() to fit the finalized workflow to the full, cleaned data (prior to splitting). This will return a fitted model object that can be used to make predictions on new data.\nThis full fit can be passed to the augment() function to make predictions on the full, cleaned data. This will return a tibble with the predictions for the full data. Use the mutate() function to calculate the residuals of the predictions. The residuals are the difference between the predicted values and the actual values squared.\n\n# Fit the final model to the entire dataset\nfinal_fit &lt;- fit(final_wf, data = camels_clean)\n\n# Get predictions for the full dataset\nfull_predictions &lt;- augment(final_fit, new_data = camels_clean)\n\n#Calculate residuals\nfull_predictions &lt;- full_predictions %&gt;%\n  mutate(residual = (logQmean - .pred)^2)\n\n#Use ggplot2 to create a map of the predictions.\nus_map &lt;- map_data(\"state\")\n\npredic_plot&lt;-ggplot(full_predictions, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = .pred), size = 3, alpha = 0.8) +\n  scale_color_viridis_c(option = \"C\") +  # You can switch palettes here\n  coord_fixed(1.3) +  # keep geographic aspect ratio\n  labs(\n    title = \"Predicted logQmean Across CONUS\",\n    x = \"Longitude\",\n    y = \"Latitude\",\n    color = \"Prediction\"\n  ) +\n  theme_minimal()\n\n#use ggplot to creat map of residuals\n\nresiduals_plot&lt;-  ggplot(full_predictions, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = residual), size = 3, alpha = 0.8) +\n  scale_color_viridis_c(option = \"magma\") +\n  coord_fixed(1.3) +\n  labs(\n    title = \"Residuals (Squared Error)\",\n    x = \"Longitude\",\n    y = \"Latitude\",\n    color = \"Residual\"\n  ) +\n  theme_minimal()\n  \n\n# Combine the two maps into a single image using patchwork\ncombined_plot &lt;- predic_plot | residuals_plot \n\ncombined_plot"
  }
]